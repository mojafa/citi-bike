The goal of the finnhub-streaming-data-pipeline project is to build a scalable and fault-tolerant streaming data pipeline for processing real-time financial data from the Finnhub API.

 The pipeline is designed to collect, process, and store financial data in real-time, and make it available for further analysis and reporting.
 
As a data engineering problem, the project involves the following tasks:

Collecting real-time financial data from the Finnhub API and sending it to a Kafka topic.
Processing the data in real-time using Apache Spark.
Storing the processed data in a scalable and fault-tolerant data storage system such as Google Cloud Storage and BigQuery.
Using dbt to model and transform the data.
Using Looker to visualize and report on the processed data.
Using Prefect to manage and orchestrate the pipeline components.
The project involves working with several data engineering technologies such as Kafka, Spark, GCS, BQ, dbt, Looker, and Prefect to build a complete end-to-end streaming data pipeline.

As you work on each step of the project, you will encounter various data engineering challenges such as data ingestion, data processing, data storage, data modeling, data transformation, data visualization, and pipeline orchestration. The project will allow you to apply your data engineering skills and knowledge to solve real-world data engineering problems.
